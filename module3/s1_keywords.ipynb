{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x109783670>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/tmp\"\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimer le nombre de fichiers identifiés\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', '1945_clean.txt', '1969_clean.txt', '1945.txt', '1969.txt']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les dix premiers fichiers\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1945_clean.txt'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choisir un fichier\n",
    "this_file = files[1]\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soir samedi decembre achat brillants buoùx argenterie monnaies obtnptotr général rorlogerte achat maurice lemnnnler midi carburant fume encrasse concessionnaire saturne tué fiancée bruxelles achat brillant bijoux argenterie monnaies bonnet passage souterrain botter nord hausse bijoux brillants monnaies atgen telles ûsbntres tableaux achat ricard tue bofanlquë brüxeljes hsm ijl pfucan erux bkiqu acier smeffielo viate wilovj étu prix spéciaux grossistes essayez gaines corsets àni gorge ceintures m'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path,this_file), 'r').read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ecrire agence rossel', 4.925865547642655e-10),\n",
       " ('ecr agence rossel', 1.3285019909327935e-09),\n",
       " ('ecr rossel dem', 3.6910942517564395e-09),\n",
       " ('agence rossel dem', 3.854738430025336e-09),\n",
       " ('rue royale bruxelles', 5.193092408890776e-09),\n",
       " ('ecrire rossel dem', 7.04826911506971e-09),\n",
       " ('bruxelles ecrire rossel', 7.106913643911179e-09),\n",
       " ('rossel dem bon', 7.63018809801017e-09),\n",
       " ('bruxelles ecr rossel', 8.039098256356346e-09),\n",
       " ('rossel rue royale', 9.894518796777533e-09),\n",
       " ('ecrire rossel dame', 1.2199322253251991e-08),\n",
       " ('ecr rossel cherche', 1.2259844260677818e-08),\n",
       " ('place ecr rossel', 1.478768973664778e-08),\n",
       " ('offres agence rossel', 1.6200208885566726e-08),\n",
       " ('ecr rossel demande', 1.6544703787016737e-08),\n",
       " ('tél écr rossel', 1.6822670696580376e-08),\n",
       " ('agence rossel vendre', 1.8285885824378676e-08),\n",
       " ('prix ecr rossel', 1.865065089969089e-08),\n",
       " ('ecr rossel vendre', 1.876018356852558e-08),\n",
       " ('ecr rossel ach', 1.9032870505764654e-08),\n",
       " ('tient lieu part', 1.957819778691753e-08),\n",
       " ('agence rossel cherche', 1.9916477923061418e-08),\n",
       " ('rossel ach maison', 1.9931645488393902e-08),\n",
       " ('ecrire rossel ach', 2.019110589692446e-08),\n",
       " ('ecr rossel bon', 2.0286072410458167e-08),\n",
       " ('rossel dem jne', 2.1100078341659976e-08),\n",
       " ('bon état rue', 2.2032846974733064e-08),\n",
       " ('ecrire rossel cherche', 2.2295829400086462e-08),\n",
       " ('ecr rossel maison', 2.26087812605741e-08),\n",
       " ('ecrire rossel maison', 2.3984583369760258e-08),\n",
       " ('dem bon serv', 2.4058200450944754e-08),\n",
       " ('ecrire rossel jeune', 2.4284803606607237e-08),\n",
       " ('prétentions agence rossel', 2.437918118591745e-08),\n",
       " ('agence rossel bruxelles', 2.4486928217147073e-08),\n",
       " ('agence rossel bon', 2.471638521854398e-08),\n",
       " ('dem bon ouvrier', 2.481534872717282e-08),\n",
       " ('références agence rossel', 2.577067632430093e-08),\n",
       " ('agence rossel demande', 2.6877332538750022e-08),\n",
       " ('réf prêt rossel', 2.6959288109692784e-08),\n",
       " ('ecrire rossel louer', 2.767698453387578e-08),\n",
       " ('dem trav rossel', 2.8031146981433523e-08),\n",
       " ('bruxelles demande bon', 2.915210026783307e-08),\n",
       " ('ecrire tél rossel', 3.024477170955909e-08),\n",
       " ('rossel vente soir', 3.025339131138818e-08),\n",
       " ('bruxelles ecrire agence', 3.0272394185071625e-08),\n",
       " ('ecr rossel louer', 3.043755810498817e-08),\n",
       " ('bruxelles bonne maison', 3.051557674955042e-08),\n",
       " ('ordre ecr rossel', 3.063942534823616e-08),\n",
       " ('ecr réf rossel', 3.073034796801668e-08),\n",
       " ('francs ecr rossel', 3.0992103938695816e-08)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire la même opération sur tous les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', '1945_clean.txt', '1969_clean.txt', '1945.txt', '1969.txt']\n",
      "1945.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(f)\n\u001b[1;32m      4\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, f), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m keywords \u001b[39m=\u001b[39m kw_extractor\u001b[39m.\u001b[39;49mextract_keywords(text)\n\u001b[1;32m      6\u001b[0m kept \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m kw, score \u001b[39min\u001b[39;00m keywords:\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/yake/yake.py:64\u001b[0m, in \u001b[0;36mKeywordExtractor.extract_keywords\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[1;32m     63\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m dc \u001b[39m=\u001b[39m DataCore(text\u001b[39m=\u001b[39;49mtext, stopword_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstopword_set, windowsSize\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindowsSize, n\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn)\n\u001b[1;32m     65\u001b[0m dc\u001b[39m.\u001b[39mbuild_single_terms_features(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m     66\u001b[0m dc\u001b[39m.\u001b[39mbuild_mult_terms_features(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/yake/datarepresentation.py:30\u001b[0m, in \u001b[0;36mDataCore.__init__\u001b[0;34m(self, text, stopword_set, windowsSize, n, tagsToDiscard, exclude)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfreq_ns[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstopword_set \u001b[39m=\u001b[39m stopword_set\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(text, windowsSize, n)\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/yake/datarepresentation.py:50\u001b[0m, in \u001b[0;36mDataCore._build\u001b[0;34m(self, text, windowsSize, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, text, windowsSize, n):\n\u001b[1;32m     49\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_filter(text)\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str \u001b[39m=\u001b[39m [ [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m split_contractions(web_tokenizer(s)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(split_multi(text)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39mstrip()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str)\n\u001b[1;32m     52\u001b[0m     pos_text \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/yake/datarepresentation.py:50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, text, windowsSize, n):\n\u001b[1;32m     49\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_filter(text)\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str \u001b[39m=\u001b[39m [ [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m split_contractions(web_tokenizer(s)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(split_multi(text)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39mstrip()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str)\n\u001b[1;32m     52\u001b[0m     pos_text \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/segtok/tokenizer.py:306\u001b[0m, in \u001b[0;36mweb_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@_matches\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39m    (?<=^|[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms<\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m])            # visual border\u001b[39m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweb_tokenizer\u001b[39m(sentence):\n\u001b[1;32m    302\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m i, span \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(web_tokenizer\u001b[39m.\u001b[39msplit(sentence))\n\u001b[1;32m    307\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m ((span,) \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m word_tokenizer(unescape(span)))]\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/segtok/tokenizer.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@_matches\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39m    (?<=^|[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms<\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m])            # visual border\u001b[39m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweb_tokenizer\u001b[39m(sentence):\n\u001b[1;32m    302\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m i, span \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(web_tokenizer\u001b[39m.\u001b[39msplit(sentence))\n\u001b[0;32m--> 307\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m ((span,) \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m word_tokenizer(unescape(span)))]\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/segtok/tokenizer.py:237\u001b[0m, in \u001b[0;36mword_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m pruned \u001b[39m=\u001b[39m HYPHENATED_LINEBREAK\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m, sentence)\n\u001b[0;32m--> 237\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m span \u001b[39min\u001b[39;00m space_tokenizer(pruned) \u001b[39mfor\u001b[39;00m\n\u001b[1;32m    238\u001b[0m           token \u001b[39min\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39msplit(span) \u001b[39mif\u001b[39;00m token]\n\u001b[1;32m    240\u001b[0m \u001b[39m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mreversed\u001b[39m(tokens[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]), \u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/e.ulb/2022_MA2_STIC/1_quadri/tac/tac/tac_venv/lib/python3.10/site-packages/segtok/tokenizer.py:238\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m pruned \u001b[39m=\u001b[39m HYPHENATED_LINEBREAK\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m, sentence)\n\u001b[1;32m    237\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m span \u001b[39min\u001b[39;00m space_tokenizer(pruned) \u001b[39mfor\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m           token \u001b[39min\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39;49msplit(span) \u001b[39mif\u001b[39;00m token]\n\u001b[1;32m    240\u001b[0m \u001b[39m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mreversed\u001b[39m(tokens[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]), \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(files)\n",
    "for f in sorted(files)[1:]:\n",
    "    print(f)\n",
    "    text = open(os.path.join(data_path, f), 'r').read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "22eb57bff7284fa477302938a752c329165cd0000dc5945798efe1987041d9d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
